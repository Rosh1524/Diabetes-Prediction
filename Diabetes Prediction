  # -*- coding: utf-8 -*-
"""DiabetsPredection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1svg1fAVqbdQTnfdturgXVFSWNb7xc08l
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix, mean_squared_error, accuracy_score

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

# Load and prepare the dataset
my_df = pd.read_csv('/content/diabetes_prediction_dataset.csv')
X = my_df.iloc[:, 0:8].values
Y = my_df.iloc[:, -1].values

# Encoding and scaling
lb = LabelEncoder()
sc = StandardScaler()
X[:, 0] = lb.fit_transform(X[:, 0])
X[:, [1, 5, 6, 7]] = sc.fit_transform(X[:, [1, 5, 6, 7]])

# One-hot encoding for feature 4
onehotencoder = ColumnTransformer([("encoder", OneHotEncoder(), [4])], remainder="passthrough")
X = np.array(onehotencoder.fit_transform(X))

# Splitting dataset
x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.80, random_state=42)

# ----------------------- SVM -------------------------
svcmodel = SVC(kernel='rbf', C=1, random_state=42)
svcmodel.fit(x_train, y_train)  # This is the long-running step

y_predict = svcmodel.predict(x_test)

print('SVM Confusion Matrix:\n', confusion_matrix(y_test, y_predict))
sns.heatmap(confusion_matrix(y_test, y_predict), annot=True, fmt='3g')
plt.title("SVM Confusion Matrix")
plt.show()

# ---------------- Random Forest ----------------------
rfc = RandomForestClassifier(max_depth=5, n_estimators=100, max_features=10, random_state=42)
rfc.fit(x_train, y_train)
y_pred = rfc.predict(x_test)
print("Random Forest Accuracy:", rfc.score(x_test, y_test))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='3g')
plt.title("Random Forest Confusion Matrix")
plt.show()

# ---------------- Decision Tree ----------------------
DTC = DecisionTreeClassifier(max_leaf_nodes=10, random_state=42)
DTC.fit(x_train, y_train)
predictions = DTC.predict(x_test)
print("Decision Tree MSE:", mean_squared_error(y_test, predictions))
sns.heatmap(confusion_matrix(y_test, predictions), annot=True, fmt='3g')
plt.title("Decision Tree Confusion Matrix")
plt.show()

# ---------------- Naive Bayes ------------------------
clf_nb = GaussianNB()
clf_nb.fit(x_train, y_train)
y_pred_nb = clf_nb.predict(x_test)
print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))
sns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='3g')
plt.title("Naive Bayes Confusion Matrix")
plt.show()

# ---------------- KNN -------------------------------
knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
knn.fit(x_train, y_train)
y_pred_knn = knn.predict(x_test)
print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, fmt='3g')
plt.title("KNN Confusion Matrix")
plt.show()

# ------------- Logistic Regression + GridSearchCV ------------
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear']
}

log_reg = LogisticRegression(random_state=42)
grid_search = GridSearchCV(log_reg, param_grid, cv=5)
grid_search.fit(x_train, y_train)

print("Best Hyperparameters:", grid_search.best_params_)
print("Best Cross-Validation Score:", grid_search.best_score_)

y_pred_lr = grid_search.predict(x_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='3g')
plt.title("Logistic Regression Confusion Matrix")
plt.show()

def preprocess_and_predict(df_input):
    # Step 1: Label Encode 'gender'
    df_input['gender'] = gender_encoder.transform(df_input['gender'])

    # Step 2: OneHotEncode 'smoking_history'
    smoking_ohe = smoking_encoder.transform(df_input[['smoking_history']])


    # Step 3: Drop 'smoking_history' column (it's now encoded) and scale numeric features
    df_input = df_input.drop('smoking_history', axis=1)

    # Step 4: Scale numerical columns
    df_input[['bmi', 'HbA1c_level', 'blood_glucose_level']] = scaler.transform(
        df_input[['bmi', 'HbA1c_level', 'blood_glucose_level']]
    )

    # Step 5: Combine with one-hot encoded smoking data
    final_input = np.hstack((smoking_ohe, df_input.values))

    # Step 6: Predict
    prediction = grid_search.predict(final_input)
    return "Diabetic" if prediction[0] == 1 else "Not Diabetic"

sample_input_df = pd.DataFrame([{
    'gender': 'Male',
    'age': 45,
    'hypertension': 0,
    'heart_disease': 0,
    'smoking_history': 'never',
    'bmi': 28.5,
    'HbA1c_level': 6.1,
    'blood_glucose_level': 145
}])

print("Real-Time Prediction:", preprocess_and_predict(sample_input_df))
